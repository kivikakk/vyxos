<!DOCTYPE html>
<html lang="en" data-theme="dark-poole">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      Tüüpiautomaat: automated testing of a compiler and target against each other &middot; kivikakk.ee
    
  </title>

  <link rel="stylesheet" href="/styles.css">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/ashe-icon-144.png">
  <link rel="shortcut icon" href="/assets/favicon.ico">
  <link rel="alternate" type="application/atom+xml" title="kivikakk.ee" href="/atom.xml">

  <meta name="theme-color" content="#3E2349">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Tüüpiautomaat: automated testing of a compiler and target against each other" />
<meta name="author" content="Asherah Connor" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I’m currently working on a bit of a long-winded project to recapture QuickBASIC, the environment I learned to program in, in a bit of a novel way, while remaining highly faithful to the original. (I actually started a related project 9 years ago (!), and I’ll be able to reuse some of what I did back then!) The new project involves compiling the BASIC down into a bytecode suitable for a stack machine, and an implementation of such a stack machine. Importantly, there’ll soon be a second implementation which runs on a different architecture entirely: namely, it’ll be an implementation of this architecture on an FPGA. So the machine needs to be reasonably easy to implement in hardware — if it simplifies the gateware design, I’ll happily take tradeoffs that involve making the compiler more complicated, or the ISA more verbose. The ISA started out with opcodes like: pub const Opcode = enum(u8) { PUSH_IMM_INTEGER = 0x01, PUSH_IMM_LONG = 0x02, PUSH_IMM_SINGLE = 0x03, PUSH_IMM_DOUBLE = 0x04, PUSH_IMM_STRING = 0x05, PUSH_VARIABLE = 0x0a, // [...] OPERATOR_ADD = 0xd0, OPERATOR_MULTIPLY = 0xd1, OPERATOR_NEGATE = 0xd2, // [...] }; For the binary operations, the stack machine would pop off two elements, and then look at the types of those elements to determine how to add them. This is easy to do in Zig, but this is giving our core a lot of work to do — especially when we consider how extensive the rules involved are: Ideally, the executing machine doesn’t ever need to check the type of a value on the stack before doing something with it — the bytecode itself can describe whatever needs to happen instead. In other words, when compiling this1: a% = 42 &#39; This is an INTEGER (-32768..32767). b&amp; = 32800 &#39; This is a LONG (-2147483648..2147483647). c% = b&amp; - a% &#39; This should result in c% = 32758. A smaller a% would result in overflow. … instead of having an op stream like this: PUSH_IMM_INTEGER(42) LET(0) PUSH_IMM_LONG(32800) LET(1) PUSH_VARIABLE(1) PUSH_VARIABLE(0) OPERATOR_SUBTRACT // runtime determines it must promote RHS and do LONG subtraction CAST_INTEGER // runtime determines it has a LONG to demote LET(2) — and so leaving the runtime environment to decide when to promote, or demote, or coerce, and when to worry about under/overflow — we instead have this: PUSH_IMM_INTEGER(42) LET(0) PUSH_IMM_LONG(32800) LET(1) PUSH_VARIABLE(1) PUSH_VARIABLE(0) PROMOTE_INTEGER_LONG // compiler knows we&#39;re about to do LONG subtraction OPERATOR_SUBTRACT_LONG COERCE_LONG_INTEGER // compiler knows we&#39;re assigning to an INTEGER LET(2) A proliferation of opcodes results, but crucially it means most operations execute unconditionally, resulting in a far simpler design when it comes to putting this on a chip. The upshot of this design, however, is that the compiler needs a far greater degree of awareness and ability: It needs to be aware of what types the arguments to the binary operation are. In the example above, we have a LONG and an INTEGER. This appears trivial, but we need to keep in mind that the arguments can be arbitrary expressions. We need this in order to determine the opcode that gets emitted for the operation, and to know if any operand reconciliation is necessary. It needs to be able to reconcile different types of operands, where necessary. BASIC’s rules here are simple: we coerce the lesser-precision value to the greater precision. In effect the rule is INTEGER &lt; LONG &lt; SINGLE &lt; DOUBLE. STRING is not compatible with any other type — there’s no &quot;x&quot; * 3 = &quot;xxx&quot; here. It needs to be aware of what type a binary operation’s result will be in. I started out with the simple rule that the result will be of the same type of the (reconciled) operands. This worked fine when I just had addition, subtraction and multiplication; above, we add a LONG and an INTEGER, the INTEGER gets promoted to a LONG, and the result is a LONG. Division broke this assumption, and resulted in the motivation for this write-up. It needs to be able to pass that information up the compilation stack. Having calculated the result is a LONG, we need to return that information to the procedure that compiled this expression, as it may make decisions based on what’s left on the stack after evaluating it — such as in the first dot-point here, or when assigning to a variable (which may be of any type, and so require a specific coercion). This all kind of Just Worked, right up until I implemented division. I discovered QuickBASIC actually has floating and integer division! Young me was not aware of the backslash “\” operator (or any need for it, I suppose). Floating division always returns a floating-point number, even when the inputs are integral. Integer division always returns an integral, even when the inputs are floating. The precision of the types returned in turn depends on that of the input types. operands fdiv “/” idiv “\” INTEGER SINGLE INTEGER LONG DOUBLE LONG SINGLE SINGLE INTEGER DOUBLE DOUBLE LONG Output types of each division operation given (reconciled) input type. This presented a problem for the existing compiler, which assumed the result would be the same type of the operands: having divided two INTEGERs, for example, the bytecode emitted assumed there would be an INTEGER left on the stack, and so further emitted code would carry that assumption. Upon realising how division was supposed to work, the first place I made the change was in the actual stack machine: correct the behaviour by performing floating division when floating division was asked for, regardless of the operand type. Thus dividing two INTEGERs pushed a SINGLE. The (Zig) machine then promptly asserted upon hitting any operation on that value at all, expecting an INTEGER there instead. (The future gateware implementation would probably not assert, and instead produce confusing garbage.) And so opened up a new kind of bug to look out for: a mismatch between (a) the assumptions made by the compiler about the effects on the stack of the operations it was emitting, and (b) the actual effects on the stack produced by those operations running on the stack machine. Rather than just some isolated tests (though short of creating some whole contract interface between compiler and machine — maybe later), why not be thorough while using the best witness for behaviour there is? Namely, the compiler and stack machine themselves: test &quot;compiler and stack machine agree on binop expression types&quot; { for (std.meta.tags(Expr.Op)) |op| { for (std.meta.tags(ty.Type)) |tyLhs| { for (std.meta.tags(ty.Type)) |tyRhs| { var c = try Compiler.init(testing.allocator, null); defer c.deinit(); const compilerTy = c.compileExpr(.{ .binop = .{ .lhs = &amp;Expr.init(Expr.Payload.oneImm(tyLhs), .{}), .op = loc.WithRange(Expr.Op).init(op, .{}), .rhs = &amp;Expr.init(Expr.Payload.oneImm(tyRhs), .{}), } }) catch |err| switch (err) { Error.TypeMismatch =&gt; continue, // keelatud eine else =&gt; return err, }; var m = stack.Machine(stack.TestEffects).init( testing.allocator, try stack.TestEffects.init(), null, ); defer m.deinit(); const code = try c.buf.toOwnedSlice(testing.allocator); defer testing.allocator.free(code); try m.run(code); try testing.expectEqual(1, m.stack.items.len); try testing.expectEqual(m.stack.items[0].type(), compilerTy); } } } } We go as follows: For all binary operations, enumerate all permutations of left- and right-hand side types. For each such triple, try compiling the binary operation with the “one”-value2 of each type as its operands. (For integrals, it’s literally the number one.) If we get a type error from this attempt, skip it — we don’t care that we can’t divide a DOUBLE by a STRING, or whatever. If not, the compileExpr method returns to us the type of what it believes the result to be. This is the same information used elsewhere in the compiler to guide opcode and coercion decisions. Create a stack machine, and run the compiled code. Seeing as we didn’t actually compile a full statement, we expect the result of the operation to be left sitting on the stack. (Normally a statement would ultimately consume what’s been pushed.) Assert that the type of the runtime value left on the stack is the same as what the compiler expects! I love how much this solution takes care of itself. While it lacks the self-descriptive power of a more coupled approach to designing the compiler and runtime, it lets the implementations remain quite clear and straightforward to read. It also lets them stand alone, which is handy when a second implementation of the runtime part is forthcoming, and is (by necessity) in a completely different language environment. There was greater value than just from floats, of course: relational operators like equal “=”, not equal “&lt;&gt;”, etc. all return INTEGERs 0 or -1, but can operate with any combination of numeric types, or with pairs of STRINGs. Bitwise operators like AND, OR, XOR, IMP (!), etc. can operate with any combination of numeric types, but coerce any floats to integrals before doing so. I bet there’ll be more to uncover, too. Hope this was fun to read! I will get some real BASIC highlighting up here by the next post on this! &#8617; I first used each type’s zero value, but then we get division by zero errors while trying to execute the code! &#8617;" />
<meta property="og:description" content="I’m currently working on a bit of a long-winded project to recapture QuickBASIC, the environment I learned to program in, in a bit of a novel way, while remaining highly faithful to the original. (I actually started a related project 9 years ago (!), and I’ll be able to reuse some of what I did back then!) The new project involves compiling the BASIC down into a bytecode suitable for a stack machine, and an implementation of such a stack machine. Importantly, there’ll soon be a second implementation which runs on a different architecture entirely: namely, it’ll be an implementation of this architecture on an FPGA. So the machine needs to be reasonably easy to implement in hardware — if it simplifies the gateware design, I’ll happily take tradeoffs that involve making the compiler more complicated, or the ISA more verbose. The ISA started out with opcodes like: pub const Opcode = enum(u8) { PUSH_IMM_INTEGER = 0x01, PUSH_IMM_LONG = 0x02, PUSH_IMM_SINGLE = 0x03, PUSH_IMM_DOUBLE = 0x04, PUSH_IMM_STRING = 0x05, PUSH_VARIABLE = 0x0a, // [...] OPERATOR_ADD = 0xd0, OPERATOR_MULTIPLY = 0xd1, OPERATOR_NEGATE = 0xd2, // [...] }; For the binary operations, the stack machine would pop off two elements, and then look at the types of those elements to determine how to add them. This is easy to do in Zig, but this is giving our core a lot of work to do — especially when we consider how extensive the rules involved are: Ideally, the executing machine doesn’t ever need to check the type of a value on the stack before doing something with it — the bytecode itself can describe whatever needs to happen instead. In other words, when compiling this1: a% = 42 &#39; This is an INTEGER (-32768..32767). b&amp; = 32800 &#39; This is a LONG (-2147483648..2147483647). c% = b&amp; - a% &#39; This should result in c% = 32758. A smaller a% would result in overflow. … instead of having an op stream like this: PUSH_IMM_INTEGER(42) LET(0) PUSH_IMM_LONG(32800) LET(1) PUSH_VARIABLE(1) PUSH_VARIABLE(0) OPERATOR_SUBTRACT // runtime determines it must promote RHS and do LONG subtraction CAST_INTEGER // runtime determines it has a LONG to demote LET(2) — and so leaving the runtime environment to decide when to promote, or demote, or coerce, and when to worry about under/overflow — we instead have this: PUSH_IMM_INTEGER(42) LET(0) PUSH_IMM_LONG(32800) LET(1) PUSH_VARIABLE(1) PUSH_VARIABLE(0) PROMOTE_INTEGER_LONG // compiler knows we&#39;re about to do LONG subtraction OPERATOR_SUBTRACT_LONG COERCE_LONG_INTEGER // compiler knows we&#39;re assigning to an INTEGER LET(2) A proliferation of opcodes results, but crucially it means most operations execute unconditionally, resulting in a far simpler design when it comes to putting this on a chip. The upshot of this design, however, is that the compiler needs a far greater degree of awareness and ability: It needs to be aware of what types the arguments to the binary operation are. In the example above, we have a LONG and an INTEGER. This appears trivial, but we need to keep in mind that the arguments can be arbitrary expressions. We need this in order to determine the opcode that gets emitted for the operation, and to know if any operand reconciliation is necessary. It needs to be able to reconcile different types of operands, where necessary. BASIC’s rules here are simple: we coerce the lesser-precision value to the greater precision. In effect the rule is INTEGER &lt; LONG &lt; SINGLE &lt; DOUBLE. STRING is not compatible with any other type — there’s no &quot;x&quot; * 3 = &quot;xxx&quot; here. It needs to be aware of what type a binary operation’s result will be in. I started out with the simple rule that the result will be of the same type of the (reconciled) operands. This worked fine when I just had addition, subtraction and multiplication; above, we add a LONG and an INTEGER, the INTEGER gets promoted to a LONG, and the result is a LONG. Division broke this assumption, and resulted in the motivation for this write-up. It needs to be able to pass that information up the compilation stack. Having calculated the result is a LONG, we need to return that information to the procedure that compiled this expression, as it may make decisions based on what’s left on the stack after evaluating it — such as in the first dot-point here, or when assigning to a variable (which may be of any type, and so require a specific coercion). This all kind of Just Worked, right up until I implemented division. I discovered QuickBASIC actually has floating and integer division! Young me was not aware of the backslash “\” operator (or any need for it, I suppose). Floating division always returns a floating-point number, even when the inputs are integral. Integer division always returns an integral, even when the inputs are floating. The precision of the types returned in turn depends on that of the input types. operands fdiv “/” idiv “\” INTEGER SINGLE INTEGER LONG DOUBLE LONG SINGLE SINGLE INTEGER DOUBLE DOUBLE LONG Output types of each division operation given (reconciled) input type. This presented a problem for the existing compiler, which assumed the result would be the same type of the operands: having divided two INTEGERs, for example, the bytecode emitted assumed there would be an INTEGER left on the stack, and so further emitted code would carry that assumption. Upon realising how division was supposed to work, the first place I made the change was in the actual stack machine: correct the behaviour by performing floating division when floating division was asked for, regardless of the operand type. Thus dividing two INTEGERs pushed a SINGLE. The (Zig) machine then promptly asserted upon hitting any operation on that value at all, expecting an INTEGER there instead. (The future gateware implementation would probably not assert, and instead produce confusing garbage.) And so opened up a new kind of bug to look out for: a mismatch between (a) the assumptions made by the compiler about the effects on the stack of the operations it was emitting, and (b) the actual effects on the stack produced by those operations running on the stack machine. Rather than just some isolated tests (though short of creating some whole contract interface between compiler and machine — maybe later), why not be thorough while using the best witness for behaviour there is? Namely, the compiler and stack machine themselves: test &quot;compiler and stack machine agree on binop expression types&quot; { for (std.meta.tags(Expr.Op)) |op| { for (std.meta.tags(ty.Type)) |tyLhs| { for (std.meta.tags(ty.Type)) |tyRhs| { var c = try Compiler.init(testing.allocator, null); defer c.deinit(); const compilerTy = c.compileExpr(.{ .binop = .{ .lhs = &amp;Expr.init(Expr.Payload.oneImm(tyLhs), .{}), .op = loc.WithRange(Expr.Op).init(op, .{}), .rhs = &amp;Expr.init(Expr.Payload.oneImm(tyRhs), .{}), } }) catch |err| switch (err) { Error.TypeMismatch =&gt; continue, // keelatud eine else =&gt; return err, }; var m = stack.Machine(stack.TestEffects).init( testing.allocator, try stack.TestEffects.init(), null, ); defer m.deinit(); const code = try c.buf.toOwnedSlice(testing.allocator); defer testing.allocator.free(code); try m.run(code); try testing.expectEqual(1, m.stack.items.len); try testing.expectEqual(m.stack.items[0].type(), compilerTy); } } } } We go as follows: For all binary operations, enumerate all permutations of left- and right-hand side types. For each such triple, try compiling the binary operation with the “one”-value2 of each type as its operands. (For integrals, it’s literally the number one.) If we get a type error from this attempt, skip it — we don’t care that we can’t divide a DOUBLE by a STRING, or whatever. If not, the compileExpr method returns to us the type of what it believes the result to be. This is the same information used elsewhere in the compiler to guide opcode and coercion decisions. Create a stack machine, and run the compiled code. Seeing as we didn’t actually compile a full statement, we expect the result of the operation to be left sitting on the stack. (Normally a statement would ultimately consume what’s been pushed.) Assert that the type of the runtime value left on the stack is the same as what the compiler expects! I love how much this solution takes care of itself. While it lacks the self-descriptive power of a more coupled approach to designing the compiler and runtime, it lets the implementations remain quite clear and straightforward to read. It also lets them stand alone, which is handy when a second implementation of the runtime part is forthcoming, and is (by necessity) in a completely different language environment. There was greater value than just from floats, of course: relational operators like equal “=”, not equal “&lt;&gt;”, etc. all return INTEGERs 0 or -1, but can operate with any combination of numeric types, or with pairs of STRINGs. Bitwise operators like AND, OR, XOR, IMP (!), etc. can operate with any combination of numeric types, but coerce any floats to integrals before doing so. I bet there’ll be more to uncover, too. Hope this was fun to read! I will get some real BASIC highlighting up here by the next post on this! &#8617; I first used each type’s zero value, but then we get division by zero errors while trying to execute the code! &#8617;" />
<link rel="canonical" href="https://kivikakk.ee/zig/2024/08/06/t%C3%BC%C3%BCpiautomaat/" />
<meta property="og:url" content="https://kivikakk.ee/zig/2024/08/06/t%C3%BC%C3%BCpiautomaat/" />
<meta property="og:site_name" content="kivikakk.ee" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-08-06T00:00:00+10:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tüüpiautomaat: automated testing of a compiler and target against each other" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Asherah Connor","url":"https://kivikakk.ee"},"dateModified":"2024-08-06T00:00:00+10:00","datePublished":"2024-08-06T00:00:00+10:00","description":"I’m currently working on a bit of a long-winded project to recapture QuickBASIC, the environment I learned to program in, in a bit of a novel way, while remaining highly faithful to the original. (I actually started a related project 9 years ago (!), and I’ll be able to reuse some of what I did back then!) The new project involves compiling the BASIC down into a bytecode suitable for a stack machine, and an implementation of such a stack machine. Importantly, there’ll soon be a second implementation which runs on a different architecture entirely: namely, it’ll be an implementation of this architecture on an FPGA. So the machine needs to be reasonably easy to implement in hardware — if it simplifies the gateware design, I’ll happily take tradeoffs that involve making the compiler more complicated, or the ISA more verbose. The ISA started out with opcodes like: pub const Opcode = enum(u8) { PUSH_IMM_INTEGER = 0x01, PUSH_IMM_LONG = 0x02, PUSH_IMM_SINGLE = 0x03, PUSH_IMM_DOUBLE = 0x04, PUSH_IMM_STRING = 0x05, PUSH_VARIABLE = 0x0a, // [...] OPERATOR_ADD = 0xd0, OPERATOR_MULTIPLY = 0xd1, OPERATOR_NEGATE = 0xd2, // [...] }; For the binary operations, the stack machine would pop off two elements, and then look at the types of those elements to determine how to add them. This is easy to do in Zig, but this is giving our core a lot of work to do — especially when we consider how extensive the rules involved are: Ideally, the executing machine doesn’t ever need to check the type of a value on the stack before doing something with it — the bytecode itself can describe whatever needs to happen instead. In other words, when compiling this1: a% = 42 &#39; This is an INTEGER (-32768..32767). b&amp; = 32800 &#39; This is a LONG (-2147483648..2147483647). c% = b&amp; - a% &#39; This should result in c% = 32758. A smaller a% would result in overflow. … instead of having an op stream like this: PUSH_IMM_INTEGER(42) LET(0) PUSH_IMM_LONG(32800) LET(1) PUSH_VARIABLE(1) PUSH_VARIABLE(0) OPERATOR_SUBTRACT // runtime determines it must promote RHS and do LONG subtraction CAST_INTEGER // runtime determines it has a LONG to demote LET(2) — and so leaving the runtime environment to decide when to promote, or demote, or coerce, and when to worry about under/overflow — we instead have this: PUSH_IMM_INTEGER(42) LET(0) PUSH_IMM_LONG(32800) LET(1) PUSH_VARIABLE(1) PUSH_VARIABLE(0) PROMOTE_INTEGER_LONG // compiler knows we&#39;re about to do LONG subtraction OPERATOR_SUBTRACT_LONG COERCE_LONG_INTEGER // compiler knows we&#39;re assigning to an INTEGER LET(2) A proliferation of opcodes results, but crucially it means most operations execute unconditionally, resulting in a far simpler design when it comes to putting this on a chip. The upshot of this design, however, is that the compiler needs a far greater degree of awareness and ability: It needs to be aware of what types the arguments to the binary operation are. In the example above, we have a LONG and an INTEGER. This appears trivial, but we need to keep in mind that the arguments can be arbitrary expressions. We need this in order to determine the opcode that gets emitted for the operation, and to know if any operand reconciliation is necessary. It needs to be able to reconcile different types of operands, where necessary. BASIC’s rules here are simple: we coerce the lesser-precision value to the greater precision. In effect the rule is INTEGER &lt; LONG &lt; SINGLE &lt; DOUBLE. STRING is not compatible with any other type — there’s no &quot;x&quot; * 3 = &quot;xxx&quot; here. It needs to be aware of what type a binary operation’s result will be in. I started out with the simple rule that the result will be of the same type of the (reconciled) operands. This worked fine when I just had addition, subtraction and multiplication; above, we add a LONG and an INTEGER, the INTEGER gets promoted to a LONG, and the result is a LONG. Division broke this assumption, and resulted in the motivation for this write-up. It needs to be able to pass that information up the compilation stack. Having calculated the result is a LONG, we need to return that information to the procedure that compiled this expression, as it may make decisions based on what’s left on the stack after evaluating it — such as in the first dot-point here, or when assigning to a variable (which may be of any type, and so require a specific coercion). This all kind of Just Worked, right up until I implemented division. I discovered QuickBASIC actually has floating and integer division! Young me was not aware of the backslash “\\” operator (or any need for it, I suppose). Floating division always returns a floating-point number, even when the inputs are integral. Integer division always returns an integral, even when the inputs are floating. The precision of the types returned in turn depends on that of the input types. operands fdiv “/” idiv “\\” INTEGER SINGLE INTEGER LONG DOUBLE LONG SINGLE SINGLE INTEGER DOUBLE DOUBLE LONG Output types of each division operation given (reconciled) input type. This presented a problem for the existing compiler, which assumed the result would be the same type of the operands: having divided two INTEGERs, for example, the bytecode emitted assumed there would be an INTEGER left on the stack, and so further emitted code would carry that assumption. Upon realising how division was supposed to work, the first place I made the change was in the actual stack machine: correct the behaviour by performing floating division when floating division was asked for, regardless of the operand type. Thus dividing two INTEGERs pushed a SINGLE. The (Zig) machine then promptly asserted upon hitting any operation on that value at all, expecting an INTEGER there instead. (The future gateware implementation would probably not assert, and instead produce confusing garbage.) And so opened up a new kind of bug to look out for: a mismatch between (a) the assumptions made by the compiler about the effects on the stack of the operations it was emitting, and (b) the actual effects on the stack produced by those operations running on the stack machine. Rather than just some isolated tests (though short of creating some whole contract interface between compiler and machine — maybe later), why not be thorough while using the best witness for behaviour there is? Namely, the compiler and stack machine themselves: test &quot;compiler and stack machine agree on binop expression types&quot; { for (std.meta.tags(Expr.Op)) |op| { for (std.meta.tags(ty.Type)) |tyLhs| { for (std.meta.tags(ty.Type)) |tyRhs| { var c = try Compiler.init(testing.allocator, null); defer c.deinit(); const compilerTy = c.compileExpr(.{ .binop = .{ .lhs = &amp;Expr.init(Expr.Payload.oneImm(tyLhs), .{}), .op = loc.WithRange(Expr.Op).init(op, .{}), .rhs = &amp;Expr.init(Expr.Payload.oneImm(tyRhs), .{}), } }) catch |err| switch (err) { Error.TypeMismatch =&gt; continue, // keelatud eine else =&gt; return err, }; var m = stack.Machine(stack.TestEffects).init( testing.allocator, try stack.TestEffects.init(), null, ); defer m.deinit(); const code = try c.buf.toOwnedSlice(testing.allocator); defer testing.allocator.free(code); try m.run(code); try testing.expectEqual(1, m.stack.items.len); try testing.expectEqual(m.stack.items[0].type(), compilerTy); } } } } We go as follows: For all binary operations, enumerate all permutations of left- and right-hand side types. For each such triple, try compiling the binary operation with the “one”-value2 of each type as its operands. (For integrals, it’s literally the number one.) If we get a type error from this attempt, skip it — we don’t care that we can’t divide a DOUBLE by a STRING, or whatever. If not, the compileExpr method returns to us the type of what it believes the result to be. This is the same information used elsewhere in the compiler to guide opcode and coercion decisions. Create a stack machine, and run the compiled code. Seeing as we didn’t actually compile a full statement, we expect the result of the operation to be left sitting on the stack. (Normally a statement would ultimately consume what’s been pushed.) Assert that the type of the runtime value left on the stack is the same as what the compiler expects! I love how much this solution takes care of itself. While it lacks the self-descriptive power of a more coupled approach to designing the compiler and runtime, it lets the implementations remain quite clear and straightforward to read. It also lets them stand alone, which is handy when a second implementation of the runtime part is forthcoming, and is (by necessity) in a completely different language environment. There was greater value than just from floats, of course: relational operators like equal “=”, not equal “&lt;&gt;”, etc. all return INTEGERs 0 or -1, but can operate with any combination of numeric types, or with pairs of STRINGs. Bitwise operators like AND, OR, XOR, IMP (!), etc. can operate with any combination of numeric types, but coerce any floats to integrals before doing so. I bet there’ll be more to uncover, too. Hope this was fun to read! I will get some real BASIC highlighting up here by the next post on this! &#8617; I first used each type’s zero value, but then we get division by zero errors while trying to execute the code! &#8617;","headline":"Tüüpiautomaat: automated testing of a compiler and target against each other","mainEntityOfPage":{"@type":"WebPage","@id":"https://kivikakk.ee/zig/2024/08/06/t%C3%BC%C3%BCpiautomaat/"},"url":"https://kivikakk.ee/zig/2024/08/06/t%C3%BC%C3%BCpiautomaat/"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body>
    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home">kivikakk.ee</a>

          <nav class="nav">
            
            <small><a href="/">Home</a></small>
            
            <small><a href="/about/">About</a></small>
            
            <small><a href="/index/">Index</a></small>
            
            <small><a href="/atom.xml">RSS</a></small>
            
          </nav>
        </h3>
      </header>

      <main>
        <article class="post">
  <div class="my-id">
    <img src="/assets/mia-icon.jpg" width="64" height="64">
  </div>
  <h1 class="post-title">Tüüpiautomaat: automated testing of a compiler and target against each other</h1>
  <time datetime="2024-08-06T00:00:00+10:00" class="post-date">06 Aug 2024</time>
  <p>I’m currently working on a bit of a <a href="https://sr.ht/~kivikakk/ava">long-winded project</a> to recapture
<a href="https://en.wikipedia.org/wiki/QuickBASIC">QuickBASIC</a>, the environment I learned to program in, in a bit of
a novel way, while remaining highly faithful to the original. (I actually
started a related project <a href="https://github.com/kivikakk/kyuubey">9 years ago</a> (!), and I’ll be able to reuse
some of what I did back then!)</p>

<p>The new project involves compiling the BASIC down into a bytecode suitable for
a stack machine, and an implementation of such a stack machine. Importantly,
there’ll soon be a second implementation which runs on a different architecture
entirely: namely, it’ll be an implementation <em>of</em> this architecture on an FPGA.
So the machine needs to be reasonably easy to implement in hardware — if it
simplifies the gateware design, I’ll happily take tradeoffs that involve making the
compiler more complicated, or the ISA more verbose.</p>

<p>The ISA started out with opcodes like:</p>

<div class="language-zig highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">const</span> <span class="n">Opcode</span> <span class="o">=</span> <span class="k">enum</span><span class="p">(</span><span class="kt">u8</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">PUSH_IMM_INTEGER</span> <span class="o">=</span> <span class="mi">0x01</span><span class="p">,</span>
    <span class="n">PUSH_IMM_LONG</span> <span class="o">=</span> <span class="mi">0x02</span><span class="p">,</span>
    <span class="n">PUSH_IMM_SINGLE</span> <span class="o">=</span> <span class="mi">0x03</span><span class="p">,</span>
    <span class="n">PUSH_IMM_DOUBLE</span> <span class="o">=</span> <span class="mi">0x04</span><span class="p">,</span>
    <span class="n">PUSH_IMM_STRING</span> <span class="o">=</span> <span class="mi">0x05</span><span class="p">,</span>
    <span class="n">PUSH_VARIABLE</span> <span class="o">=</span> <span class="mi">0x0a</span><span class="p">,</span>
    <span class="c">// [...]</span>
    <span class="n">OPERATOR_ADD</span> <span class="o">=</span> <span class="mi">0xd0</span><span class="p">,</span>
    <span class="n">OPERATOR_MULTIPLY</span> <span class="o">=</span> <span class="mi">0xd1</span><span class="p">,</span>
    <span class="n">OPERATOR_NEGATE</span> <span class="o">=</span> <span class="mi">0xd2</span><span class="p">,</span>
    <span class="c">// [...]</span>
<span class="p">};</span>
</code></pre></div></div>

<p>For the binary operations, the stack machine would pop off two elements, and then
look at the types of those elements to determine how to add them. This is easy to
do in Zig, but this is giving our core a lot of work to do — especially when we
consider how extensive the rules involved are:</p>

<p><img src="/assets/post-img/typeconversions.png" alt="Screenshot of QuickBASIC 4.5 open to the &quot;Type Conversions&quot; help page.
It's scrolled some way down and yet the screen is full of text describing how
operands are converted to the same degree of precision, and further how result
types may affect that." /></p>

<p>Ideally, the executing machine doesn’t <em>ever</em> need to check the type of a value
on the stack before doing something with it — the bytecode itself can describe
whatever needs to happen instead.</p>

<p>In other words, when compiling this<sup id="fnref:highlighting" role="doc-noteref"><a href="#fn:highlighting" class="footnote" rel="footnote">1</a></sup>:</p>

<div class="language-zig highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="o">%</span> <span class="o">=</span> <span class="mi">42</span>       <span class="err">'</span> <span class="n">This</span> <span class="n">is</span> <span class="n">an</span> <span class="n">INTEGER</span> <span class="p">(</span><span class="o">-</span><span class="mi">32768</span><span class="o">..</span><span class="mi">32767</span><span class="p">).</span>
<span class="py">b</span><span class="o">&amp;</span> <span class="o">=</span> <span class="mi">32800</span>    <span class="err">'</span> <span class="n">This</span> <span class="n">is</span> <span class="n">a</span> <span class="n">LONG</span> <span class="p">(</span><span class="o">-</span><span class="mi">2147483648</span><span class="o">..</span><span class="mi">2147483647</span><span class="p">).</span>
<span class="py">c</span><span class="o">%</span> <span class="o">=</span> <span class="n">b</span><span class="o">&amp;</span> <span class="o">-</span> <span class="n">a</span><span class="o">%</span>  <span class="err">'</span> <span class="n">This</span> <span class="n">should</span> <span class="n">result</span> <span class="n">in</span> <span class="n">c</span><span class="o">%</span> <span class="o">=</span> <span class="mi">32758</span><span class="p">.</span>  <span class="py">A</span> <span class="n">smaller</span> <span class="n">a</span><span class="o">%</span> <span class="n">would</span> <span class="n">result</span> <span class="n">in</span> <span class="n">overflow</span><span class="o">.</span>
</code></pre></div></div>

<p>… instead of having an op stream like this:</p>

<div class="language-zig highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PUSH_IMM_INTEGER</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">LET</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">PUSH_IMM_LONG</span><span class="p">(</span><span class="mi">32800</span><span class="p">)</span>
<span class="n">LET</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">PUSH_VARIABLE</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">PUSH_VARIABLE</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">OPERATOR_SUBTRACT</span>     <span class="c">// runtime determines it must promote RHS and do LONG subtraction</span>
<span class="n">CAST_INTEGER</span>          <span class="c">// runtime determines it has a LONG to demote</span>
<span class="n">LET</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>— and so leaving the runtime environment to decide when to promote, or demote,
or coerce, and when to worry about under/overflow — we instead have this:</p>

<div class="language-zig highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PUSH_IMM_INTEGER</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">LET</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">PUSH_IMM_LONG</span><span class="p">(</span><span class="mi">32800</span><span class="p">)</span>
<span class="n">LET</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">PUSH_VARIABLE</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">PUSH_VARIABLE</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">PROMOTE_INTEGER_LONG</span>     <span class="c">// compiler knows we're about to do LONG subtraction</span>
<span class="n">OPERATOR_SUBTRACT_LONG</span>
<span class="n">COERCE_LONG_INTEGER</span>      <span class="c">// compiler knows we're assigning to an INTEGER</span>
<span class="n">LET</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>A proliferation of opcodes results, but crucially it means most operations
execute unconditionally, resulting in a far simpler design when it comes to
putting this on a chip.</p>

<p>The upshot of this design, however, is that the compiler needs a far greater degree
of awareness and ability:</p>

<ul>
  <li>
    <p>It needs to be aware of what types the arguments to the binary operation are.</p>

    <p>In the example above, we have a LONG and an INTEGER. This appears trivial, but
we need to keep in mind that the arguments can be arbitrary expressions.</p>

    <p>We need this in order to determine the opcode that gets emitted for the
operation, and to know if any operand reconciliation is necessary.</p>
  </li>
  <li>
    <p>It needs to be able to reconcile different types of operands, where necessary.</p>

    <p>BASIC’s rules here are simple: we coerce the lesser-precision
value to the greater precision. In effect the rule is
INTEGER &lt; LONG &lt; SINGLE &lt; DOUBLE.</p>

    <p>STRING is not compatible with any other type — there’s no <nobr><code>"x" * 3 = "xxx"</code></nobr> here.</p>
  </li>
  <li>
    <p>It needs to be aware of what type a binary operation’s result will be in.</p>

    <p>I started out with the simple rule that the result will be of the same type
of the (reconciled) operands. This worked fine when I just had addition,
subtraction and multiplication; above, we add a LONG and an INTEGER, the INTEGER
gets promoted to a LONG, and the result is a LONG.</p>

    <p>Division broke this assumption, and resulted in the motivation for this
write-up.</p>
  </li>
  <li>
    <p>It needs to be able to pass that information up the compilation stack.</p>

    <p>Having calculated the result is a LONG, we need to return that information to
the procedure that compiled this expression, as it may make decisions based
on what’s left on the stack after evaluating it — such as in the first
dot-point here, or when assigning to a variable (which may be of any type, and
so require a specific coercion).</p>
  </li>
</ul>

<p>This all kind of Just Worked, right up until I implemented division. I
discovered QuickBASIC actually has floating <em>and</em> integer division! Young me
was not aware of the backslash “<code class="language-plaintext highlighter-rouge">\</code>” operator (or any need for it, I suppose).
Floating division always returns a floating-point number, even when the inputs
are integral. Integer division always returns an integral, even when the inputs
are floating. The precision of the types returned in turn depends on that of the
input types.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">operands</th>
      <th style="text-align: right">fdiv “<code class="language-plaintext highlighter-rouge">/</code>”</th>
      <th style="text-align: right">idiv “<code class="language-plaintext highlighter-rouge">\</code>”</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">INTEGER</td>
      <td style="text-align: right">SINGLE</td>
      <td style="text-align: right">INTEGER</td>
    </tr>
    <tr>
      <td style="text-align: right">LONG</td>
      <td style="text-align: right">DOUBLE</td>
      <td style="text-align: right">LONG</td>
    </tr>
    <tr>
      <td style="text-align: right">SINGLE</td>
      <td style="text-align: right">SINGLE</td>
      <td style="text-align: right">INTEGER</td>
    </tr>
    <tr>
      <td style="text-align: right">DOUBLE</td>
      <td style="text-align: right">DOUBLE</td>
      <td style="text-align: right">LONG</td>
    </tr>
  </tbody>
</table>

<p><center><em>Output types of each division operation given (reconciled) input type.</em></center></p>

<p>This presented a problem for the existing compiler, which assumed the result
would be the same type of the operands: having divided two INTEGERs, for
example, the bytecode emitted <em>assumed</em> there would be an INTEGER left on the
stack, and so further emitted code would carry that assumption.</p>

<p>Upon realising how division was supposed to work, the first place I made the
change was in the actual stack machine: correct the behaviour by performing
floating division when floating division was asked for, regardless of the
operand type. Thus dividing two INTEGERs pushed a SINGLE. The (Zig) machine then
promptly asserted upon hitting any operation on that value at all, expecting an
INTEGER there instead. (The future gateware implementation would probably not
assert, and instead produce confusing garbage.)</p>

<p>And so opened up a new kind of bug to look out for: a mismatch between (a)
the assumptions made by the compiler about the effects on the stack of the
operations it was emitting, and (b) the actual effects on the stack produced by
those operations running on the stack machine.</p>

<p>Rather than just some isolated tests (though short of creating some whole
contract interface between compiler and machine — maybe later), why not be
thorough while using the best witness for behaviour there is? Namely, the
compiler and stack machine themselves:</p>

<div class="language-zig highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">test</span> <span class="s">"compiler and stack machine agree on binop expression types"</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">std</span><span class="p">.</span><span class="py">meta</span><span class="p">.</span><span class="nf">tags</span><span class="p">(</span><span class="n">Expr</span><span class="p">.</span><span class="py">Op</span><span class="p">))</span> <span class="p">|</span><span class="n">op</span><span class="p">|</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">std</span><span class="p">.</span><span class="py">meta</span><span class="p">.</span><span class="nf">tags</span><span class="p">(</span><span class="n">ty</span><span class="p">.</span><span class="py">Type</span><span class="p">))</span> <span class="p">|</span><span class="n">tyLhs</span><span class="p">|</span> <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">std</span><span class="p">.</span><span class="py">meta</span><span class="p">.</span><span class="nf">tags</span><span class="p">(</span><span class="n">ty</span><span class="p">.</span><span class="py">Type</span><span class="p">))</span> <span class="p">|</span><span class="n">tyRhs</span><span class="p">|</span> <span class="p">{</span>
                <span class="k">var</span> <span class="n">c</span> <span class="o">=</span> <span class="k">try</span> <span class="n">Compiler</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">testing</span><span class="p">.</span><span class="py">allocator</span><span class="p">,</span> <span class="kc">null</span><span class="p">);</span>
                <span class="k">defer</span> <span class="n">c</span><span class="p">.</span><span class="nf">deinit</span><span class="p">();</span>

                <span class="k">const</span> <span class="n">compilerTy</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">compileExpr</span><span class="p">(</span><span class="o">.</span><span class="p">{</span> <span class="p">.</span><span class="py">binop</span> <span class="o">=</span> <span class="o">.</span><span class="p">{</span>
                    <span class="p">.</span><span class="py">lhs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">Expr</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">Expr</span><span class="p">.</span><span class="py">Payload</span><span class="p">.</span><span class="nf">oneImm</span><span class="p">(</span><span class="n">tyLhs</span><span class="p">),</span> <span class="o">.</span><span class="p">{}),</span>
                    <span class="p">.</span><span class="py">op</span> <span class="o">=</span> <span class="n">loc</span><span class="p">.</span><span class="nf">WithRange</span><span class="p">(</span><span class="n">Expr</span><span class="p">.</span><span class="py">Op</span><span class="p">).</span><span class="nf">init</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">.</span><span class="p">{}),</span>
                    <span class="p">.</span><span class="py">rhs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">Expr</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">Expr</span><span class="p">.</span><span class="py">Payload</span><span class="p">.</span><span class="nf">oneImm</span><span class="p">(</span><span class="n">tyRhs</span><span class="p">),</span> <span class="o">.</span><span class="p">{}),</span>
                <span class="p">}</span> <span class="p">})</span> <span class="k">catch</span> <span class="p">|</span><span class="n">err</span><span class="p">|</span> <span class="k">switch</span> <span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="p">{</span>
                    <span class="n">Error</span><span class="p">.</span><span class="py">TypeMismatch</span> <span class="o">=&gt;</span> <span class="k">continue</span><span class="p">,</span> <span class="c">// keelatud eine</span>
                    <span class="k">else</span> <span class="o">=&gt;</span> <span class="k">return</span> <span class="n">err</span><span class="p">,</span>
                <span class="p">};</span>

                <span class="k">var</span> <span class="n">m</span> <span class="o">=</span> <span class="n">stack</span><span class="p">.</span><span class="nf">Machine</span><span class="p">(</span><span class="n">stack</span><span class="p">.</span><span class="py">TestEffects</span><span class="p">).</span><span class="nf">init</span><span class="p">(</span>
                    <span class="n">testing</span><span class="p">.</span><span class="py">allocator</span><span class="p">,</span>
                    <span class="k">try</span> <span class="n">stack</span><span class="p">.</span><span class="py">TestEffects</span><span class="p">.</span><span class="nf">init</span><span class="p">(),</span>
                    <span class="kc">null</span><span class="p">,</span>
                <span class="p">);</span>
                <span class="k">defer</span> <span class="n">m</span><span class="p">.</span><span class="nf">deinit</span><span class="p">();</span>

                <span class="k">const</span> <span class="n">code</span> <span class="o">=</span> <span class="k">try</span> <span class="n">c</span><span class="p">.</span><span class="py">buf</span><span class="p">.</span><span class="nf">toOwnedSlice</span><span class="p">(</span><span class="n">testing</span><span class="p">.</span><span class="py">allocator</span><span class="p">);</span>
                <span class="k">defer</span> <span class="n">testing</span><span class="p">.</span><span class="py">allocator</span><span class="p">.</span><span class="nf">free</span><span class="p">(</span><span class="n">code</span><span class="p">);</span>

                <span class="k">try</span> <span class="n">m</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">code</span><span class="p">);</span>

                <span class="k">try</span> <span class="n">testing</span><span class="p">.</span><span class="nf">expectEqual</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">.</span><span class="py">stack</span><span class="p">.</span><span class="py">items</span><span class="p">.</span><span class="py">len</span><span class="p">);</span>
                <span class="k">try</span> <span class="n">testing</span><span class="p">.</span><span class="nf">expectEqual</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="py">stack</span><span class="p">.</span><span class="py">items</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="k">type</span><span class="p">(),</span> <span class="n">compilerTy</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>We go as follows:</p>

<ul>
  <li>
    <p>For all binary operations, enumerate all permutations of left- and right-hand
side types.</p>
  </li>
  <li>
    <p>For each such triple, try compiling the binary operation with the “one”-value<sup id="fnref:one" role="doc-noteref"><a href="#fn:one" class="footnote" rel="footnote">2</a></sup>
of each type as its operands. (For integrals, it’s literally the number one.)</p>

    <ul>
      <li>
        <p>If we get a type error from this attempt, skip it — we don’t care that we can’t
divide a DOUBLE by a STRING, or whatever.</p>
      </li>
      <li>
        <p>If not, the <code class="language-plaintext highlighter-rouge">compileExpr</code> method returns to us the type of what it believes
the result to be. This is the same information used elsewhere in the
compiler to guide opcode and coercion decisions.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Create a stack machine, and run the compiled code.</p>

    <ul>
      <li>Seeing as we didn’t actually compile a full statement, we expect the result
of the operation to be left sitting on the stack. (Normally a statement
would ultimately consume what’s been pushed.)</li>
    </ul>
  </li>
  <li>
    <p>Assert that the type of the runtime value left on the stack is the same as
what the compiler expects!</p>
  </li>
</ul>

<p>I love how much this solution takes care of itself. While it lacks the
self-descriptive power of a more coupled approach to designing the compiler and
runtime, it lets the implementations remain quite clear and straightforward to
read. It also lets them stand alone, which is handy when a second implementation
of the runtime part is forthcoming, and is (by necessity) in a completely
different language environment.</p>

<p>There was greater value than just from floats, of course: relational operators
like equal “<code class="language-plaintext highlighter-rouge">=</code>”, not equal “<code class="language-plaintext highlighter-rouge">&lt;&gt;</code>”, etc. all return INTEGERs 0 or -1, but
can operate with any combination of numeric types, or with pairs of STRINGs.
Bitwise operators like <code class="language-plaintext highlighter-rouge">AND</code>, <code class="language-plaintext highlighter-rouge">OR</code>, <code class="language-plaintext highlighter-rouge">XOR</code>, <code class="language-plaintext highlighter-rouge">IMP</code> (!), etc. can operate with any
combination of numeric types, but coerce any floats to integrals before doing
so. I bet there’ll be more to uncover, too.</p>

<p>Hope this was fun to read!</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:highlighting" role="doc-endnote">
      <p>I will get some real BASIC highlighting up here by the next
             post on this! <a href="#fnref:highlighting" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:one" role="doc-endnote">
      <p>I first used each type’s zero value, but then we get division by zero errors while trying
    to execute the code! <a href="#fnref:one" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</article>


<aside class="related">
  <h3>Related posts</h3>
  <ul class="related-posts">
    
      <li>
        <a href="/2024/10/13/led/">
          Led
          <small><time datetime="2024-10-13T18:13:00+11:00">13 Oct 2024</time></small>
        </a>
      </li>
    
      <li>
        <a href="/2024/10/13/rss-autodiscovery/">
          RSS autodiscovery
          <small><time datetime="2024-10-13T00:00:00+11:00">13 Oct 2024</time></small>
        </a>
      </li>
    
      <li>
        <a href="/2024/07/27/kalakarp/">
          Kalakarp
          <small><time datetime="2024-07-27T00:00:00+10:00">27 Jul 2024</time></small>
        </a>
      </li>
    
  </ul>
</aside>


      </main>

      <footer class="footer">
        <a href="/archive/">Archived blog</a>
        <small>Xx</small>
      </footer>
    </div>
  </body>
</html>
